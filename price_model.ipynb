{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-8d3f69723a3f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'PYTHONHASHSEED'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'10000'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "import pyximport\n",
    "pyximport.install()\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "os.environ['PYTHONHASHSEED'] = '10000'\n",
    "np.random.seed(10001)\n",
    "random.seed(10002)\n",
    "session_conf = tf.ConfigProto(intra_op_parallelism_threads=5, inter_op_parallelism_threads=1)\n",
    "from keras import backend\n",
    "tf.set_random_seed(10003)\n",
    "backend.set_session(tf.Session(graph=tf.get_default_graph(), config=session_conf))\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Input, Dropout, Dense, concatenate, GRU, Embedding, Flatten, Activation, BatchNormalization, PReLU\n",
    "from keras.initializers import he_uniform\n",
    "from keras.layers import Conv1D\n",
    "from keras.layers import GlobalMaxPooling1D, GlobalAveragePooling1D\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.models import Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################################\n",
    "    #GLOBAL VARIABLES\n",
    "path = '../input/'\n",
    "split = -1#1400000 # use -1 for submission, otherwise tha value of split is the number of instances in train\n",
    "cores = 4\n",
    "max_text_length=60\n",
    "min_df_one=5\n",
    "min_df_bi=5\n",
    "\n",
    "def clean_str(text):\n",
    "    try:\n",
    "        text = ' '.join( [w for w in text.split()[:max_text_length]] )        \n",
    "        text = text.lower()\n",
    "        text = re.sub(u\\\"é\\\", u\\\"e\\\", text)\n",
    "        text = re.sub(u\\\"ē\\\", u\\\"e\\\", text)\n",
    "        text = re.sub(u\\\"è\\\", u\\\"e\\\", text)\n",
    "        text = re.sub(u\\\"ê\\\", u\\\"e\\\", text)\n",
    "        text = re.sub(u\\\"à\\\", u\\\"a\\\", text)\n",
    "        text = re.sub(u\\\"â\\\", u\\\"a\\\", text)\n",
    "        text = re.sub(u\\\"ô\\\", u\\\"o\\\", text)\n",
    "        text = re.sub(u\\\"ō\\\", u\\\"o\\\", text)\n",
    "        text = re.sub(u\\\"ü\\\", u\\\"u\\\", text)\n",
    "        text = re.sub(u\\\"ï\\\", u\\\"i\\\", text)\n",
    "        text = re.sub(u\\\"ç\\\", u\\\"c\\\", text)\n",
    "        text = re.sub(u\\\"\\\\u2019\\\", u\\\"'\\\", text)\n",
    "        text = re.sub(u\\\"\\\\xed\\\", u\\\"i\\\", text)\n",
    "        text = re.sub(u\\\"w\\\\/\\\", u\\\" with \\\", text)\n",
    "        \n",
    "        text = re.sub(u\\\"[^a-z0-9]\\\", \\\" \\\", text)\n",
    "        text = u\\\" \\\".join(re.split('(\\\\d+)',text) )\n",
    "        text = re.sub( u\\\"\\\\s+\\\", u\\\" \\\", text ).strip()\n",
    "        text = ''.join(text)\n",
    "    except:\n",
    "        text = np.NaN\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data( ):\n",
    "    print ('LOAD 1.4M ROWS FOR TRAIN')\n",
    "    df_train = pd.read_csv(path+'train.tsv', sep='\\\\t', encoding='utf-8')\n",
    "    df_train['item_condition_id'].fillna(2, inplace=True)\n",
    "    df_train['shipping'].fillna(0, inplace=True)\n",
    "    if split>0:\n",
    "        df_train = df_train.loc[:split].reset_index(drop=True)\n",
    "    df_train = df_train.loc[df_train.price>0].reset_index(drop=True)\n",
    "    df_train['price'] = np.log1p(df_train['price']).astype(np.float32)\n",
    "    df_train.drop('train_id', axis=1, inplace=True)\n",
    "    return df_train\n",
    "    \n",
    "def create_count_features(df_data):\n",
    "    def lg(text):\n",
    "        text = [x for x in text.split() if x!='']\n",
    "        return len(text)\n",
    "    df_data['nb_words_item_description'] = df_data['item_description'].apply(lg).astype(np.uint16)\n",
    "    \n",
    "def parallelize_dataframe(df, func):\n",
    "    df_split = np.array_split(df, cores)\n",
    "    pool = Pool(cores)\n",
    "    df = pd.concat(pool.map(func, df_split))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return df\n",
    "def clean_str_df(df):\n",
    "    return df.apply( lambda s : clean_str(s))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(df_data, train=True):\n",
    "    print ('Prepare data....')\n",
    "    \n",
    "    def fill_brand_name(x):\n",
    "        try:\n",
    "            k=[]\n",
    "            for n in [4,3,2,1]:\n",
    "                temp =  [' '.join(xi) for xi in ngrams(x.split(' '), n) if ' '.join(xi) in   brand_names  ] \n",
    "                if len(temp)>0:\n",
    "                    k = k+temp\n",
    "            if len(k) > 0:\n",
    "                return k[0]\n",
    "            else:\n",
    "                return np.NaN\n",
    "        except:\n",
    "            return np.NaN\n",
    "        \n",
    "    def fill_cat(x, i, new=False):\n",
    "        try:\n",
    "            if new:\n",
    "                return x.split('/')[i-1].strip()\n",
    "            else:\n",
    "                return ' '.join( x.split('/') ).strip()\n",
    "        except:\n",
    "            return ''\n",
    "            \n",
    "    df_data['name'].fillna('', inplace=True)\n",
    "    df_data['item_description'].fillna('', inplace=True)\n",
    "    df_data['item_description'] = df_data['item_description'].apply(lambda x : x.replace('No description yet',''))\n",
    "    \n",
    "    #create 3 categories and remove / from category name and replace nan\n",
    "    df_data['category_name'].fillna('//', inplace=True)\n",
    "    df_data['category1'] = df_data.category_name.apply(lambda x : x.split('/')[0].strip())\n",
    "    df_data['category2'] = df_data.category_name.apply(lambda x : x.split('/')[1].strip())\n",
    "    df_data['category3'] = df_data.category_name.apply(lambda x : x.split('/')[2].strip())\n",
    "    df_data['category_name'] = df_data['category_name'].apply( lambda x : ' '.join( x.split('/') ).strip() )\n",
    "\n",
    "    create_count_features(df_data)     \n",
    "    df_data['nb_words_item_description'] /= max_text_length\n",
    "\n",
    "    df_data['brand_name'] = parallelize_dataframe(df_data['brand_name'], clean_str_df)  \n",
    "    df_data['name'] = parallelize_dataframe(df_data['name'], clean_str_df)  \n",
    "    df_data['item_description'] = parallelize_dataframe(df_data['item_description'], clean_str_df)                                                                            \n",
    "    \n",
    "    df_data.loc[df_data['brand_name'].isnull(), 'brand_name'] = df_data.loc[df_data['brand_name'].isnull(),\n",
    "                                                                            'name'].apply(fill_brand_name)\n",
    "    df_data['brand_name'].fillna('', inplace=True)\n",
    "    \n",
    "    if train:        \n",
    "        for feat in ['brand_name', 'category_name', 'category1', 'category2', 'category3']:\n",
    "            temp = df_data[feat].unique()\n",
    "            lb = LabelEncoder()\n",
    "            df_data[feat] = lb.fit_transform(df_data[feat]).astype(np.uint16)\n",
    "            labels_dict[feat] = (lb, temp)\n",
    "    else:   \n",
    "        for feat in ['brand_name', 'category1', 'category2', 'category3', 'category_name']  :\n",
    "            idx = labels_dict[feat][1]\n",
    "            df_data.loc[ -df_data[feat].isin(idx), feat ] = ''\n",
    "            df_data[feat] = labels_dict[feat][0].transform(df_data[feat]).astype(np.uint16)\n",
    "    df_data['name_old'] = df_data['name'].copy()    \n",
    "        \n",
    "    df_data['brand_cat']  = 'cat1_'+df_data['category1'].astype(str)+' '+\\\\\n",
    "    'cat2_'+df_data['category2'].astype(str)+' '+\\\\\n",
    "    'cat3_'+df_data['category3'].astype(str)+' '+\\\\\n",
    "    'brand_'+df_data['brand_name'].astype(str) \n",
    "    \n",
    "    df_data['name']  = df_data['brand_cat']  + ' ' + df_data['name']\n",
    "    \n",
    "    df_data['name_desc']  = df_data['name'] + ' ' +\\\\\n",
    "    df_data['item_description'].apply( lambda x : ' '.join( x.split()[:5] ) )\n",
    "    \n",
    "    df_data['item_condition_id'] = df_data['item_condition_id']/5.\n",
    "    return df_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_count(text, dc):\n",
    "    text = set( text.split(' ') ) \n",
    "    for w in text:\n",
    "        dc[w]+=1\n",
    "\n",
    "def remove_low_freq(text, dc):\n",
    "    return ' '.join( [w for w in text.split() if w in dc] )\n",
    "    \n",
    "def create_bigrams(text):\n",
    "    try:\n",
    "        text = np.unique( [ wordnet_lemmatizer.lemmatize(w) for w in text.split() if w not in stop_words ] )\n",
    "        lst_bi = []\n",
    "        for combo in combinations(text, 2):\n",
    "            cb1=combo[0]+combo[1]\n",
    "            cb2=combo[1]+combo[0]\n",
    "            in_dict=False\n",
    "            if cb1 in word_count_dict_one:\n",
    "                new_word = cb1\n",
    "                in_dict=True\n",
    "            if cb2 in word_count_dict_one:\n",
    "                new_word = cb2\n",
    "                in_dict=True\n",
    "            if not in_dict:\n",
    "                new_word = combo[0]+'___'+combo[1]\n",
    "            if len(cb1)>=0:\n",
    "                lst_bi.append(new_word)\n",
    "        return ' '.join( lst_bi )\n",
    "    except:\n",
    "        return ' '\n",
    "        \n",
    "def create_bigrams_df(df):\n",
    "    return df.apply( create_bigrams )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################################################################\n",
    "############################  TRAIN PROCESSING  #####################################\n",
    "##########################################################################################################\n",
    "print('*'*50)\n",
    "labels_dict = dict()\n",
    "df_train = load_data()\n",
    "print (df_train.shape)\n",
    "brand_names = df_train.groupby('brand_name').size()  \n",
    "start_time = time.time()\n",
    "df_train = prepare_data(df_train, train=True)\n",
    "print('[{}] Finished TRAIN DATA PREPARATION'.format(time.time() - start_time))\n",
    "\n",
    "#STORE ALL WORDS  FREQUENCY and Filter\n",
    "#################################################\n",
    "start_time = time.time()\n",
    "word_count_dict_one = defaultdict(np.uint32)\n",
    "for feat in ['name','item_description' ]:\n",
    "    df_train[feat].apply(             lambda x : word_count(x, word_count_dict_one) )\n",
    "rare_words = [key for key in word_count_dict_one if  word_count_dict_one[key]<min_df_one ]\n",
    "for key in rare_words :\n",
    "    word_count_dict_one.pop(key, None)\n",
    "for feat in ['name','item_description' ]:\n",
    "    df_train[feat]      = df_train[feat].apply( lambda x : remove_low_freq(x, word_count_dict_one) )\n",
    "word_count_dict_one=dict(word_count_dict_one)\n",
    "print('[{}] Finished COUNTING WORDS FOR NAME AND DESCRIPTION...'.format(time.time() - start_time))\n",
    "\n",
    "#Create ALL 2_ways combinations (Custom Bigrams)\n",
    "#################################################\n",
    "start_time = time.time()\n",
    "word_count_dict_bi=defaultdict(np.uint32)\n",
    "def word_count_bi(text):\n",
    "    text =  text.split(' ') \n",
    "    for w in text:\n",
    "        word_count_dict_bi[w]+=1\n",
    "df_train['name_bi']      = parallelize_dataframe( df_train['name_desc'],  create_bigrams_df )\n",
    "df_train['name_bi'].apply(word_count_bi )\n",
    "rare_words = [key for key in word_count_dict_bi if  word_count_dict_bi[key]<min_df_bi ]\n",
    "for key in rare_words :\n",
    "    word_count_dict_bi.pop(key, None)\n",
    "df_train['name_bi']      = df_train['name_bi'].apply( lambda x : remove_low_freq(x, word_count_dict_bi) )\n",
    "print('[{}] Finished CREATING BIGRAMS...'.format(time.time() - start_time))\n",
    "\n",
    "#####################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "word_count_dict_bi = dict(word_count_dict_bi)\n",
    "vocabulary_one = word_count_dict_one.copy()\n",
    "vocabulary_bi = word_count_dict_bi.copy()\n",
    "for dc in [vocabulary_one,  vocabulary_bi]:\n",
    "    cpt=0\n",
    "    for key in dc:\n",
    "        dc[key]=cpt\n",
    "        cpt+=1\n",
    "print('[{}] Finished CREATING VOCABULARY ...'.format(time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_dc=dict()\n",
    "for feat in ['category1', 'category2', 'category3', 'category_name', 'brand_name'  ]:\n",
    "    mean_dc[feat] = df_train.groupby(feat)['price'].mean().astype(np.float32)\n",
    "    mean_dc[feat] /= np.max(mean_dc[feat])\n",
    "    df_train['mean_price_'+feat] = df_train[feat].map(mean_dc[feat]).astype(np.float32)\n",
    "    df_train['mean_price_'+feat].fillna( mean_dc[feat].mean(), inplace=True  )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################### vectorizers\n",
    "def tokenize(text):\n",
    "    return [w for w in text.split()]\n",
    "start_time = time.time()\n",
    "vect_name_one            = CountVectorizer(vocabulary= vocabulary_one,   dtype=np.uint8,\n",
    "                                            tokenizer=tokenize, binary=True ) \n",
    "train_name_one  = vect_name_one.fit_transform( df_train['name'] )\n",
    "print (train_name_one.shape)\n",
    "print('[{}] Finished Vectorizing Onegram Name'.format(time.time() - start_time))\n",
    "\n",
    "start_time = time.time()\n",
    "vect_item_one            = CountVectorizer(vocabulary= vocabulary_one,   dtype=np.uint8, \n",
    "                                            tokenizer=tokenize, binary=True ) \n",
    "train_item_one  = vect_item_one.fit_transform( df_train['item_description']  )\n",
    "print (train_item_one.shape)\n",
    "print('[{}] Finished Vectorizing Onegram Item Description'.format(time.time() - start_time))\n",
    "\n",
    "start_time = time.time()\n",
    "vect_name_bi           = CountVectorizer(vocabulary= vocabulary_bi,   dtype=np.uint8, \n",
    "                                            tokenizer=tokenize, binary=True ) \n",
    "train_name_bi  = vect_name_bi.fit_transform( df_train['name_bi']  )\n",
    "print (train_name_bi.shape)\n",
    "print('[{}] Finished Vectorizing BiGram Name'.format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################################################################\n",
    "    ##############################TRAIN AND PREDICT################################\n",
    "    #############################################################################################################\n",
    "    keep = ['item_condition_id', 'shipping', 'nb_words_item_description',\n",
    "           'mean_price_category1', 'mean_price_category2', 'mean_price_category3',\n",
    "            'mean_price_category_name', 'mean_price_brand_name']\n",
    "    \n",
    "    #RIDGE MODEL 1\n",
    "    dtrain_y = df_train.price.values\n",
    "    dtrain  = hstack((df_train[keep].values, train_name_one, train_item_one, train_name_bi  )).tocsr()\n",
    "    print ('RIDGE MATRIX SIZE : ',dtrain.shape)\n",
    "    start_time=time.time()\n",
    "    model_ridge_name = Ridge(alpha=20, copy_X=True, fit_intercept=True, solver='auto',\n",
    "                        max_iter=100,   normalize=False, random_state=0,  tol=0.0025)\n",
    "    model_ridge_name.fit(dtrain, dtrain_y)\n",
    "    print ('ridge time : ',time.time()-start_time)\n",
    "    del dtrain, train_name_bi, train_name_one, train_item_one\n",
    "    df_train.drop('name_bi', axis=1, inplace=True)\n",
    "    gc.collect()\n",
    "    \n",
    "    ####################################################################################################\n",
    "    #SPARSE NN MODEL\n",
    "    def tokenize(text):\n",
    "        return [ w for w in text.split()]\n",
    "    \n",
    "    start_time = time.time()\n",
    "    vect_sparse = CountVectorizer(lowercase=False, min_df=5, ngram_range=(1,2), max_features=200000,\n",
    "                                        dtype=np.uint8,       tokenizer=tokenize, strip_accents=None, binary=True )\n",
    "    vect_sparse.fit( df_train['name']+' '+df_train['item_description']  )\n",
    "    def get_keras_sparse(df):\n",
    "        X = {'sparse_data': vect_sparse.transform( df['name']+' '+df['item_description']  ) ,\n",
    "            'item_condition': np.array(df['item_condition_id']),\n",
    "            'shipping': np.array(df[\\shipping\\]),\n",
    "            'temp': np.array(df[\\mean_price_category2\\]),\n",
    "            'temp2': np.array(df[\\nb_words_item_description\\])\n",
    "        }\n",
    "        return X\n",
    "    train_keras      = get_keras_sparse(df_train)\n",
    "    \n",
    "    def sparseNN():                                             \n",
    "        sparse_data = Input( shape=[train_keras[\\sparse_data\\].shape[1]], \n",
    "            dtype = 'float32',   sparse = True, name='sparse_data')  \n",
    "    \n",
    "        item_condition = Input(shape=[1], name=\\item_condition\\)\n",
    "        shipping = Input(shape=[1], name=\\shipping\\)\n",
    "        temp = Input(shape=[1], name=\\temp\\)\n",
    "        temp2 = Input(shape=[1], name=\\temp2\\)\n",
    "        \n",
    "        x = Dense(200 , kernel_initializer=he_uniform(seed=0) )(sparse_data)    \n",
    "        x = PReLU()(x)\n",
    "        x = concatenate( [x, item_condition, shipping, temp, temp2] ) \n",
    "        x = Dense(200 , kernel_initializer=he_uniform(seed=0) )(x)\n",
    "        x = PReLU()(x)\n",
    "        x = Dense(100 , kernel_initializer=he_uniform(seed=0) )(x)\n",
    "        x = PReLU()(x)\n",
    "        x= Dense(1)(x)\n",
    "        \n",
    "        model = Model([sparse_data, item_condition, shipping,   temp, temp2],x)\n",
    "        \n",
    "        optimizer = Adam(.0011)\n",
    "        model.compile(loss=\\mse\\, optimizer=optimizer)\n",
    "        return model\n",
    "    \n",
    "    BATCH_SIZE = 2000\n",
    "    epochs = 3\n",
    "    \n",
    "    sparse_nn = sparseNN()\n",
    "    \n",
    "    print(\\Fitting SPARSE NN model ...\\)\n",
    "    \n",
    "    mean_price = np.mean(df_train.price.values)\n",
    "    \n",
    "    for ep in range(epochs):\n",
    "        BATCH_SIZE = int(BATCH_SIZE*2)\n",
    "        sparse_nn.fit(  train_keras, (df_train.price.values-mean_price), \n",
    "                          batch_size=BATCH_SIZE, epochs=1, verbose=10 )\n",
    "    \n",
    "    del train_keras\n",
    "    gc.collect\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the conv. model we are testing on the TSV[2] \n",
    "\n",
    "def conv_model(hp):\n",
    "    inputs = x \n",
    "\n",
    "    if hp.Choice(\"pooling\" + str(i), [\"max\", \"avg\"]) == \"max\":\n",
    "        x = tf.keras.layers.MaxPooling2D()(x)\n",
    "    else:\n",
    "        x = tf.keras.layers.AveragePooling2D()(x)\n",
    "\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.ReLU()(x)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2bfcb1d4df6eb12671b9d509f778d60a363494077f5d1c0f7f352516fa33f538"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
