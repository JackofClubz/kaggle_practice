{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-8d3f69723a3f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'PYTHONHASHSEED'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'10000'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "import pyximport\n",
    "pyximport.install()\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "os.environ['PYTHONHASHSEED'] = '10000'\n",
    "np.random.seed(10001)\n",
    "random.seed(10002)\n",
    "session_conf = tf.ConfigProto(intra_op_parallelism_threads=5, inter_op_parallelism_threads=1)\n",
    "from keras import backend\n",
    "tf.set_random_seed(10003)\n",
    "backend.set_session(tf.Session(graph=tf.get_default_graph(), config=session_conf))\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Input, Dropout, Dense, concatenate, GRU, Embedding, Flatten, Activation, BatchNormalization, PReLU\n",
    "from keras.initializers import he_uniform\n",
    "from keras.layers import Conv1D\n",
    "from keras.layers import GlobalMaxPooling1D, GlobalAveragePooling1D\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.models import Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################################\n",
    "    #GLOBAL VARIABLES\n",
    "path = '../input/'\n",
    "split = -1#1400000 # use -1 for submission, otherwise tha value of split is the number of instances in train\n",
    "cores = 4\n",
    "max_text_length=60\n",
    "min_df_one=5\n",
    "min_df_bi=5\n",
    "\n",
    "def clean_str(text):\n",
    "    try:\n",
    "        text = ' '.join( [w for w in text.split()[:max_text_length]] )        \n",
    "        text = text.lower()\n",
    "        text = re.sub(u\\\"é\\\", u\\\"e\\\", text)\n",
    "        text = re.sub(u\\\"ē\\\", u\\\"e\\\", text)\n",
    "        text = re.sub(u\\\"è\\\", u\\\"e\\\", text)\n",
    "        text = re.sub(u\\\"ê\\\", u\\\"e\\\", text)\n",
    "        text = re.sub(u\\\"à\\\", u\\\"a\\\", text)\n",
    "        text = re.sub(u\\\"â\\\", u\\\"a\\\", text)\n",
    "        text = re.sub(u\\\"ô\\\", u\\\"o\\\", text)\n",
    "        text = re.sub(u\\\"ō\\\", u\\\"o\\\", text)\n",
    "        text = re.sub(u\\\"ü\\\", u\\\"u\\\", text)\n",
    "        text = re.sub(u\\\"ï\\\", u\\\"i\\\", text)\n",
    "        text = re.sub(u\\\"ç\\\", u\\\"c\\\", text)\n",
    "        text = re.sub(u\\\"\\\\u2019\\\", u\\\"'\\\", text)\n",
    "        text = re.sub(u\\\"\\\\xed\\\", u\\\"i\\\", text)\n",
    "        text = re.sub(u\\\"w\\\\/\\\", u\\\" with \\\", text)\n",
    "        \n",
    "        text = re.sub(u\\\"[^a-z0-9]\\\", \\\" \\\", text)\n",
    "        text = u\\\" \\\".join(re.split('(\\\\d+)',text) )\n",
    "        text = re.sub( u\\\"\\\\s+\\\", u\\\" \\\", text ).strip()\n",
    "        text = ''.join(text)\n",
    "    except:\n",
    "        text = np.NaN\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data( ):\n",
    "    print ('LOAD 1.4M ROWS FOR TRAIN')\n",
    "    df_train = pd.read_csv(path+'train.tsv', sep='\\\\t', encoding='utf-8')\n",
    "    df_train['item_condition_id'].fillna(2, inplace=True)\n",
    "    df_train['shipping'].fillna(0, inplace=True)\n",
    "    if split>0:\n",
    "        df_train = df_train.loc[:split].reset_index(drop=True)\n",
    "    df_train = df_train.loc[df_train.price>0].reset_index(drop=True)\n",
    "    df_train['price'] = np.log1p(df_train['price']).astype(np.float32)\n",
    "    df_train.drop('train_id', axis=1, inplace=True)\n",
    "    return df_train\n",
    "    \n",
    "def create_count_features(df_data):\n",
    "    def lg(text):\n",
    "        text = [x for x in text.split() if x!='']\n",
    "        return len(text)\n",
    "    df_data['nb_words_item_description'] = df_data['item_description'].apply(lg).astype(np.uint16)\n",
    "    \n",
    "def parallelize_dataframe(df, func):\n",
    "    df_split = np.array_split(df, cores)\n",
    "    pool = Pool(cores)\n",
    "    df = pd.concat(pool.map(func, df_split))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return df\n",
    "def clean_str_df(df):\n",
    "    return df.apply( lambda s : clean_str(s))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(df_data, train=True):\n",
    "    print ('Prepare data....')\n",
    "    \n",
    "    def fill_brand_name(x):\n",
    "        try:\n",
    "            k=[]\n",
    "            for n in [4,3,2,1]:\n",
    "                temp =  [' '.join(xi) for xi in ngrams(x.split(' '), n) if ' '.join(xi) in   brand_names  ] \n",
    "                if len(temp)>0:\n",
    "                    k = k+temp\n",
    "            if len(k) > 0:\n",
    "                return k[0]\n",
    "            else:\n",
    "                return np.NaN\n",
    "        except:\n",
    "            return np.NaN\n",
    "        \n",
    "    def fill_cat(x, i, new=False):\n",
    "        try:\n",
    "            if new:\n",
    "                return x.split('/')[i-1].strip()\n",
    "            else:\n",
    "                return ' '.join( x.split('/') ).strip()\n",
    "        except:\n",
    "            return ''\n",
    "            \n",
    "    df_data['name'].fillna('', inplace=True)\n",
    "    df_data['item_description'].fillna('', inplace=True)\n",
    "    df_data['item_description'] = df_data['item_description'].apply(lambda x : x.replace('No description yet',''))\n",
    "    \n",
    "    #create 3 categories and remove / from category name and replace nan\n",
    "    df_data['category_name'].fillna('//', inplace=True)\n",
    "    df_data['category1'] = df_data.category_name.apply(lambda x : x.split('/')[0].strip())\n",
    "    df_data['category2'] = df_data.category_name.apply(lambda x : x.split('/')[1].strip())\n",
    "    df_data['category3'] = df_data.category_name.apply(lambda x : x.split('/')[2].strip())\n",
    "    df_data['category_name'] = df_data['category_name'].apply( lambda x : ' '.join( x.split('/') ).strip() )\n",
    "\n",
    "    create_count_features(df_data)     \n",
    "    df_data['nb_words_item_description'] /= max_text_length\n",
    "\n",
    "    df_data['brand_name'] = parallelize_dataframe(df_data['brand_name'], clean_str_df)  \n",
    "    df_data['name'] = parallelize_dataframe(df_data['name'], clean_str_df)  \n",
    "    df_data['item_description'] = parallelize_dataframe(df_data['item_description'], clean_str_df)                                                                            \n",
    "    \n",
    "    df_data.loc[df_data['brand_name'].isnull(), 'brand_name'] = df_data.loc[df_data['brand_name'].isnull(),\n",
    "                                                                            'name'].apply(fill_brand_name)\n",
    "    df_data['brand_name'].fillna('', inplace=True)\n",
    "    \n",
    "    if train:        \n",
    "        for feat in ['brand_name', 'category_name', 'category1', 'category2', 'category3']:\n",
    "            temp = df_data[feat].unique()\n",
    "            lb = LabelEncoder()\n",
    "            df_data[feat] = lb.fit_transform(df_data[feat]).astype(np.uint16)\n",
    "            labels_dict[feat] = (lb, temp)\n",
    "    else:   \n",
    "        for feat in ['brand_name', 'category1', 'category2', 'category3', 'category_name']  :\n",
    "            idx = labels_dict[feat][1]\n",
    "            df_data.loc[ -df_data[feat].isin(idx), feat ] = ''\n",
    "            df_data[feat] = labels_dict[feat][0].transform(df_data[feat]).astype(np.uint16)\n",
    "    df_data['name_old'] = df_data['name'].copy()    \n",
    "        \n",
    "    df_data['brand_cat']  = 'cat1_'+df_data['category1'].astype(str)+' '+\\\\\n",
    "    'cat2_'+df_data['category2'].astype(str)+' '+\\\\\n",
    "    'cat3_'+df_data['category3'].astype(str)+' '+\\\\\n",
    "    'brand_'+df_data['brand_name'].astype(str) \n",
    "    \n",
    "    df_data['name']  = df_data['brand_cat']  + ' ' + df_data['name']\n",
    "    \n",
    "    df_data['name_desc']  = df_data['name'] + ' ' +\\\\\n",
    "    df_data['item_description'].apply( lambda x : ' '.join( x.split()[:5] ) )\n",
    "    \n",
    "    df_data['item_condition_id'] = df_data['item_condition_id']/5.\n",
    "    return df_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_count(text, dc):\n",
    "    text = set( text.split(' ') ) \n",
    "    for w in text:\n",
    "        dc[w]+=1\n",
    "\n",
    "def remove_low_freq(text, dc):\n",
    "    return ' '.join( [w for w in text.split() if w in dc] )\n",
    "    \n",
    "def create_bigrams(text):\n",
    "    try:\n",
    "        text = np.unique( [ wordnet_lemmatizer.lemmatize(w) for w in text.split() if w not in stop_words ] )\n",
    "        lst_bi = []\n",
    "        for combo in combinations(text, 2):\n",
    "            cb1=combo[0]+combo[1]\n",
    "            cb2=combo[1]+combo[0]\n",
    "            in_dict=False\n",
    "            if cb1 in word_count_dict_one:\n",
    "                new_word = cb1\n",
    "                in_dict=True\n",
    "            if cb2 in word_count_dict_one:\n",
    "                new_word = cb2\n",
    "                in_dict=True\n",
    "            if not in_dict:\n",
    "                new_word = combo[0]+'___'+combo[1]\n",
    "            if len(cb1)>=0:\n",
    "                lst_bi.append(new_word)\n",
    "        return ' '.join( lst_bi )\n",
    "    except:\n",
    "        return ' '\n",
    "        \n",
    "def create_bigrams_df(df):\n",
    "    return df.apply( create_bigrams )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "86103890b8e590731de96d4c792ee598cf8a92a24bb5d00e2e611a3a1f264a81"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
